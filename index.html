<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Fast Predictive Image Registration by rkwitt</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Fast Predictive Image Registration</h1>
      <h2 class="project-tagline">Source code for X. Yang et al., &quot;Fast Predictive Image Registration&quot;</h2>
      <a href="https://github.com/rkwitt/FastPredictiveImageRegistration" class="btn">View on GitHub</a>
      <a href="https://github.com/rkwitt/FastPredictiveImageRegistration/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/rkwitt/FastPredictiveImageRegistration/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <p>This project page contains instructions to reproduce the results for our <strong>initial paper</strong>, 
as well as a tutorial on how to use this approach for your own registration tasks.</p>

<h2>
<a id="paper" class="anchor" href="#paper" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Paper</h2>

<p><strong>Fast Predictive Image Registration</strong><br>
X. Yang (UNC Chapel Hill, USA)<br>
R. Kwitt (University of Salzburg, Austria)<br>
M. Niethammer (UNC Chapel Hill, USA)<br>
MICCAI DLMI workshop (2016)    </p>

<pre lang="bibtex"><code>@inproceedings{@YKM16a,
    author    = {X. Yang and R. Kwitt and M. Niethammer},
    title     = {Fast Predictive Image Registration},
    booktitle = {DLMI},
    year      = {2016}}        
</code></pre>

<h2>
<a id="requirements" class="anchor" href="#requirements" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Requirements</h2>

<p>Our approach for momenta prediction is implemented in Torch. The actual mapping/warping of images (using the predicted momenta) is then realized using <a href="https://bitbucket.org/scicompanat/pyca">PyCA</a> and <a href="https://bitbucket.org/scicompanat/vectormomentum">VectorMomentum LDDMM</a>, both from the SCI Computational Anatomy repository on Bitbucket. Regarding the minimum system requirements, we tested our code on Linux running Ubuntu 16.04 with 24GB of main memory and a NVIDIA Titan X (sponsored by NVIDIA, running CUDA 8.0). If the code runs on your system, please let us know your configuration and we are happy to add it to our list of supported platforms/configurations.</p>

<ul>
<li><a href="http://torch.ch/">Torch</a></li>
<li><a href="https://bitbucket.org/scicompanat/pyca">PyCA</a></li>
<li><a href="https://bitbucket.org/scicompanat/vectormomentum">Vector Momentum LDDMM</a></li>
<li>MATLAB (+ <a href="https://www.mathworks.com/matlabcentral/fileexchange/41594-medical-image-processing-toolbox">Medical Image Processing Toolbox</a>)<br>
</li>
</ul>

<h2>
<a id="compilation" class="anchor" href="#compilation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compilation</h2>

<p>Lets assume that all of our code will resides under <code>/code</code>. First, we are going to install PyCA. Due to compatibility issues between the latest VectorMomentum LDDMM package (which we will use later on) and PyCA, we suggest to checkout a slightly older version of PyCA. Note, that we also need the Insight Toolkit (ITK) to be installed.</p>

<div class="highlight highlight-source-shell"><pre>apt-get install insighttoolkit4-python libinsighttoolkit4-dev <span class="pl-c"># install ITK v4</span>
<span class="pl-c1">cd</span> /code
git clone git@bitbucket.org:scicompanat/pyca.git
<span class="pl-c1">cd</span> /code/pyca
git checkout 9954dd5319efaa0ac5f58977e57acf004ad73ed7
mkdir Build
<span class="pl-c1">cd</span> Build
ccmake .. <span class="pl-c"># configure PyCA via the cmake ncurses interface</span></pre></div>

<p>On our system, we had to bump the <code>CUDA_ARCH_VERSION</code> to 20 for PyCA to compile. This can be done via the ccmake interface in the <em>Advanced Mode</em>. Also, the Python bindings did not compile with SWIG 3.0.3 (which was the version installed on our Ubuntu 16.04 system); we downgraded to swig-2.0.12 instead</p>

<div class="highlight highlight-source-shell"><pre>sudo apt-get install swig2.0
<span class="pl-c"># Note that you might have to uninstall swig3.0 if errors persist</span></pre></div>

<p>Then, running</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /code/pyca/Build
make -j4 <span class="pl-c"># -j4 will run 4 parallel jobs</span></pre></div>

<p>should compile PyCA. Finally, we set the <code>PYTHONPATH</code> as follows (we do recommend to put the following export statement into the .bashrc file, assuming that you run bash):</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-k">export</span> PYTHONPATH=<span class="pl-smi">${PYTHONPATH}</span>:PATH_TO_PYCA/Build/python_module</pre></div>

<p>where you have to (1) replace <code>PATH_TO_PYCA</code> with the name of the directory where you checked out PyCA (we assume that you built PyCA under PATH_TO_PYCA/Build, in our case <code>/code/pyca/Build/python_module</code>). Next, we are going to clone the VectorMomentum LDDMM code and set the <code>PYTHONPATH</code> accordingly:</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /code
git clone https://bitbucket.org/scicompanat/vectormomentum.git
<span class="pl-k">export</span> PYTHONPATH=<span class="pl-smi">${PYTHONPATH}</span>:/code/vectormomentum/Code/Python</pre></div>

<h2>
<a id="installation" class="anchor" href="#installation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation</h2>

<p>To install our code for momenta prediction, just clone our GitHub repository as:</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /code
git clone https://github.com/rkwitt/FastPredictiveImageRegistration.git</pre></div>

<p>In the following example, we will describe (in detail) how to actually run the code.</p>

<h2>
<a id="example-1-2d-atlas-to-image-registration" class="anchor" href="#example-1-2d-atlas-to-image-registration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EXAMPLE 1: 2D atlas-to-image registration</h2>

<p><img src="https://rkwitt.github.io/FastPredictiveImageRegistration/images/montage2D.png" alt=""></p>

<p>We provide example 2D data (i.e., 2D MR slices from the OASIS brain database) as well as a pre-trained model in the repository under the <code>data</code> directory. This directory contains</p>

<ul>
<li>100 images (transversal slices) for training</li>
<li>50 images (transversal slices) for testing</li>
<li>1 atlas image</li>
</ul>

<p>Additionally, the data directory contains computed target momenta for atlas-to-image registration which will be used for training our network (these are the momenta that we would get by running LDDMM registration). In case you don't want to use the pre-trained model for experimenting, you can train your own network for momentum prediction with the provided data as follows:</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /code/FastPredictiveImageRegistration
th main_nodiff.lua</pre></div>

<p>We will give another example later for the case when no LDDMM registrations (i.e., the momenta) are available for training.
The network definition for the particular model in this example can be found in the file <code>create_model.lua</code> (in the function <code>VAE_deformation_parallel_small_noDropout</code>). Per default, we train (using rmsprop) for 10 epochs using a batch size of 300 and 15x15 patches.</p>

<h3>
<a id="momenta-prediction-by-a-trained-model" class="anchor" href="#momenta-prediction-by-a-trained-model" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Momenta prediction by a trained model</h3>

<p>To run the (pre)-trained model for momenta prediction, we execute</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /code/FastPredictiveImageRegistration
th test_recon_nodiff.lua</pre></div>

<p>Note that this code might fail in case you do not have the Torch <code>matio</code> module installed. In that
case, you can use <code>luarocks</code> to install it with</p>

<div class="highlight highlight-source-shell"><pre>luarocks install matio</pre></div>

<p>Once, <code>test_recon_nodiff.lua</code> completes, you should see a file named <code>2D_output.mat</code> in the <code>/code/FastPredictiveImageRegistration</code> directory. </p>

<h3>
<a id="warping-images-based-on-predicted-momenta" class="anchor" href="#warping-images-based-on-predicted-momenta" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Warping images based on predicted momenta</h3>

<p>We implement the warping step as follows: first, we will convert all momenta predictions to <code>.mhd</code> files; second, we will prepare the configuration files for VectorMomentum LDDMM and eventually run geodesic shooting using the predicted momenta. Lets first start MATLAB and change to the <code>utils</code> directory:</p>

<div class="highlight highlight-source-matlab"><pre><span class="pl-k">cd</span> /code/FastPredictiveImageRegistration/utils</pre></div>

<p>Then, we edit the file <code>change_m0_to_mhd_2D.m</code> and set the corresponding paths. Per default, the output directory will be <code>/tmp/</code> and the predicted momenta files will be <code>/tmp/m_1.mhd</code> to <code>/tmp/m_50.mhd</code>, since we have N=50 test cases. Next, let us write the YAML configuration files for the VectorMomentum LDDMM code. This is done by editing <code>updateyaml.m</code>. Running <code>updateyaml</code> will then create N=50 configuration files in the output directory.</p>

<div class="highlight highlight-source-matlab"><pre><span class="pl-k">cd</span> /code/FastPredictiveImageRegistration/utils
change_m0_to_mhd_2D
updateyaml</pre></div>

<p>We can now run the VectorMomentum LDDMM code (for our first test case of this example) as follows:</p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /external/vectormomentum/Code/Python/Applications/
python CAvmGeodesicShooting.py <span class="pl-k">~</span>/deformation-prediction-code/utils/deep_network_1.yaml</pre></div>

<p>This will create the atlas image <em>warped</em> onto the source image in the directory <code>/tmp/case_1</code>. In case running the geodesic shooting code produces errors (related to plotting), just comment-out <code>GeodesicShootingPlots(g, ginv, I0, It, cf)</code> on line 151 of <code>CAvmGeodesicShooting.py</code>.</p>

<h2>
<a id="example-2-3d-atlas-to-image-registration" class="anchor" href="#example-2-3d-atlas-to-image-registration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EXAMPLE 2: 3D atlas-to-image registration</h2>

<p>We provide example training and testing 3D data from the <a href="http://www.oasis-brains.org/">OASIS</a> 
brain database (including the atlas) - 100 images for training, 50 images for testing. 
These images have been (affinely) pre-aligned by <strong>Nikhil Singh</strong>. We additionally provide 
momenta (obtained by PyCA) for all training and testing images to train (and test, resp.) our 
encoder-decoder network. </p>

<h3>
<a id="downloading-the-data" class="anchor" href="#downloading-the-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Downloading the data</h3>

<p>The training/testing data can be downloaded <a href="https://drive.google.com/drive/folders/0BxHF82gaPzgSemFFbjdDWVN3bkk?usp=sharing.">here</a>. <strong>Note</strong>: the total data size is about 10GB. We recommend to put the data into the <code>data</code> subdirectory, 
i.e., <code>/code/FastPredictiveImageRegistration/data</code> in our example so that you do not have to edit the code.</p>

<h3>
<a id="training" class="anchor" href="#training" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Training</h3>

<p>The code for training the 3D encoder-decoder network can be found in <code>main_nodiff_3D_patchprune.lua</code>, assuming
that the data has been downloaded to <code>/code/FastPredictiveImageRegistration/data</code>. Executing </p>

<div class="highlight highlight-source-shell"><pre><span class="pl-c1">cd</span> /code/FastPredictiveImageRegistration/
python main_nodiff_3D_patchprune.lua</pre></div>

<p>will train the encoder-decoder network (on our system with one NVIDIA Titan X, this takes about two days).</p>

<h2>
<a id="acknowledgments" class="anchor" href="#acknowledgments" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Acknowledgments</h2>

<p>We like to thank the creators of the OASIS brain database for freely providing the data.
The provided (preprocessed) OASIS data is subject to the same <strong>Data Usage Agreement</strong> as listed on the
<a href="http://www.oasis-brains.org/app/template/UsageAgreement.vm">OASIS webpage</a>.</p>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contact</h2>

<p>Please contact <strong>Xiao Yang</strong> for comments, suggestions or bug reports :)</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/rkwitt/FastPredictiveImageRegistration">Fast Predictive Image Registration</a> is maintained by <a href="https://github.com/rkwitt">rkwitt</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
