<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Fast Predictive Image Registration</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/prism.css" data-noprefix>

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

</head>
<body>

  <!-- For code highlighting -->
  <script src="js/prism.js"></script>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="row">
      <div class="twelve columns" style="margin-top: 10%">
        <h4>Fast Predictive Image Registration</h4>
        <h6><b>X. Yang</b>, R. Kwitt and M. Niethammer</h6>
        <a class="button" href="#">GitHub</a>
        <a class="button" href="#">Paper</a>
        <a class="button" href="#">Presentation</a>
        <img class="u-max-full-width" src="images/dlmi-embedded.svg">
        <p>This project page contains instructions to reproduce the results
        for our initial paper, as well as a tutorial on how to use this approach
        for your own registration tasks.
        </p>
      </div>
    </div>



    <div class="row">
      <div class="twelve columns" style="margin-top: 2rem">
        <h5>Paper</h5>
        <p>
        <b>Fast Predictive Image Registration</b><br>
        X. Yang (UNC Chapel Hill), R. Kwitt (Univ. of Salzburg) and M. Niethammer
         (UNC Chapel Hill)<br>
        DLMI 2016
        </p>
        <p>
        If you use this work, please <b>cite</b> the paper using the following
        BibTex entry in your paper.
        <pre><code class="language-bash">
@inproceedings{@YKM16a,
  author    = {X. Yang and R. Kwitt and M. Niethammer},
  title     = {Fast Predictive Image Registration},
  booktitle = {DLMI},
  year      = {2016}}
        </code></pre>
        </p>
      </div>
    </div>

    <div class="row">
      <div class="twelve columns" style="margin-top: 2rem">
        <h5>Requirements</h5>
        <p>
        Our approach for momenta prediction is implemented in Torch. The actual
        mapping of the source image to the atlas (using the predicted momenta) 
        is then realized using PyCA.
        </p>
        <ul>
          <li><a href="http://torch.ch">Torch</a></li>
          <li><a href="https://bitbucket.org/scicompanat/pyca">PyCA</a></li>
          <li><a href="https://bitbucket.org/scicompanat/vectormomentum">Vector Momentum LDDMM</a></li>
        </ul>
        For installation instructions for those software packages, please
        see the corresponding documentations.
        Note: due to compatibility issues between the latest Vector Momentum LDDMM package and PyCA package, we suggest checkout a old version of PyCA using the following command:
        <pre><code class="language-bash">
        git clone git@bitbucket.org:scicompanat/pyca.git
        cd ./pyca
        git checkout 9954dd5319efaa0ac5f58977e57acf004ad73ed7
        </code></pre>
      </div>
    </div>

    <div class="row">
      <div class="twelve columns" style="margin-top: 2rem">
        <h5>Installation</h5>
        <p>
        First, clone the GitHub repository as:
        <pre><code class="language-bash">
git clone https://github.com/rkwitt/FastPredictiveImageRegistration.git
        </code></pre>
      </p>
      </div>
    </div>

    <div class="row">
      <div class="twelve columns" style="margin-top: 2rem">
        <h5>A first simple (2D) example</h5>
        <img class="u-max-full-width" src="images/montage2D.png">
        <p>
        We already provide 2D example data in the repository under the <code>data</code> directory. 
        This data contains (1) 100 
        2D slices of brain MR images (see examples above), (2) target momenta for LDDMM registration
        of the images to the atlas, as well as (3)
        the corresponding atlas image slice. You can already start training a variational autoencoder 
        for momentum prediction using:
        <pre><code class="language-bash">
th main_nodiff.lua
        </code></pre>
        The network definition for this particular model can be found in the file <code>create_model.lua</code>
        in the function <code>create_model.VAE_deformation_parallel_small_noDropout</code>. Per default, we 
        train (using rmsprop) for 10 epochs using a batch size of 300 and 15x15 patches. 
        </p>
      </div>
    </div>

    <div class="row">
      <div class="twelve columns" style="margin-top: 2rem">
        <h5>Full example</h5>
        <p>
        tbd.
        </p>
      </div>
    </div>


    <div class="row">
      <div class="twelve columns" style="margin-top: 2rem">
        <h5>Data</h5>
        <p>
        A download link for the 3D data we used in the experiments will be 
        provided soon.
        </p>
      </div>
    </div>



  </div>

<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
</body>
</html>
