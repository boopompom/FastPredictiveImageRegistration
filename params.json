{
  "name": "Fast Predictive Image Registration",
  "tagline": "Source code for X. Yang et al., \"Fast Predictive Image Registration\"",
  "body": "This project page contains instructions to reproduce the results for our **initial paper**, \r\nas well as a tutorial on how to use this approach for your own registration tasks.\r\n\r\n## Paper\r\n\r\n**Fast Predictive Image Registration**    \r\nX. Yang (UNC Chapel Hill, USA)     \r\nR. Kwitt (University of Salzburg, Austria)    \r\nM. Niethammer (UNC Chapel Hill, USA)    \r\nMICCAI DLMI workshop (2016)    \r\n\r\n```bibtex\r\n@inproceedings{@YKM16a,\r\n    author    = {X. Yang and R. Kwitt and M. Niethammer},\r\n    title     = {Fast Predictive Image Registration},\r\n    booktitle = {DLMI},\r\n    year      = {2016}}        \r\n```\r\n\r\n## Requirements\r\n\r\nOur approach for momenta prediction is implemented in Torch. The actual mapping/warping of images (using the predicted momenta) is realized using [PyCA](https://bitbucket.org/scicompanat/pyca) and [VectorMomentum LDDMM](https://bitbucket.org/scicompanat/vectormomentum), both available from the SCI Computational Anatomy repository on Bitbucket. Regarding the minimum system requirements, we tested our code on Linux running Ubuntu 16.04 with 24GB of main memory and a NVIDIA Titan X (sponsored by NVIDIA, running CUDA 8.0). If the code runs on your system, please let us know your configuration and we are happy to create a list of supported platforms/configurations.\r\n\r\n* [Torch](http://torch.ch/)\r\n* [PyCA](https://bitbucket.org/scicompanat/pyca)\r\n* [Vector Momentum LDDMM](https://bitbucket.org/scicompanat/vectormomentum)\r\n* MATLAB (+ [Medical Image Processing Toolbox](https://www.mathworks.com/matlabcentral/fileexchange/41594-medical-image-processing-toolbox))    \r\n\r\n## Compilation\r\n\r\nLets assume that all of our code will reside under `/code`. First, we are going to install PyCA. Due to compatibility issues between the latest VectorMomentum LDDMM package (which we will use later on) and PyCA, we suggest to checkout a slightly older version of PyCA. Note, that we also need the Insight Toolkit (ITK) to be installed.\r\n\r\n```bash\r\napt-get install insighttoolkit4-python libinsighttoolkit4-dev # install ITK v4\r\ncd /code\r\ngit clone git@bitbucket.org:scicompanat/pyca.git\r\ncd /code/pyca\r\ngit checkout 9954dd5319efaa0ac5f58977e57acf004ad73ed7\r\nmkdir Build\r\ncd Build\r\nccmake .. # configure PyCA via the cmake ncurses interface\r\n```\r\n\r\nOn our system, we had to bump the `CUDA_ARCH_VERSION` to 20 for PyCA to compile. This can be done via the ccmake interface in the *Advanced Mode*. Also, the Python bindings did not compile with SWIG 3.0.3 (which was the version installed on our Ubuntu 16.04 system); we downgraded to swig-2.0.12 instead, e.g., with\r\n\r\n```bash\r\nsudo apt-get install swig2.0\r\n# Note that you might have to uninstall swig3.0 if errors persist\r\n```\r\n\r\nThen, running\r\n\r\n```bash\r\ncd /code/pyca/Build\r\nmake -j4 # -j4 will run 4 parallel jobs\r\n```\r\n\r\nshould compile PyCA. Finally, we set the `PYTHONPATH` as (we do recommend to put the following export statement into the .bashrc file, assuming that you are running a bash shell):\r\n\r\n```bash\r\nexport PYTHONPATH=${PYTHONPATH}:PATH_TO_PYCA/Build/python_module\r\n```\r\n\r\nwhere you have to (1) replace `PATH_TO_PYCA` with the name of the directory where you checked out PyCA (we assume that you built PyCA under PATH_TO_PYCA/Build, in our case `/code/pyca/Build/python_module`). Next, we are going to clone the VectorMomentum LDDMM code and set the `PYTHONPATH` accordingly:\r\n\r\n```bash\r\ncd /code\r\ngit clone https://bitbucket.org/scicompanat/vectormomentum.git\r\nexport PYTHONPATH=${PYTHONPATH}:/code/vectormomentum/Code/Python\r\n```\r\n\r\n## Installation\r\n\r\nTo install our encoder-decoder code, just clone our GitHub repository with\r\n\r\n```bash\r\ncd /code\r\ngit clone https://github.com/rkwitt/FastPredictiveImageRegistration.git\r\n```\r\n\r\nIn the following example, we will describe (in detail) how to actually run the code.\r\n\r\n## EXAMPLE 1: 2D atlas-to-image registration\r\n\r\n![](https://rkwitt.github.io/FastPredictiveImageRegistration/images/montage2D.png)\r\n\r\nWe provide example 2D data (i.e., 2D MR slices from the OASIS brain database) as well as a pre-trained model in the repository under the `data` directory. This directory contains\r\n\r\n* 100 images (transversal slices) for training\r\n* 50 images (transversal slices) for testing\r\n* 1 atlas image (in `data/images/2D_atlas/atlas.mhd`)\r\n\r\nAdditionally, the data directory contains computed target momenta for atlas-to-image registration which will be used for training our network (these are the momenta that we would get by running LDDMM registration). In case you don't want to use the pre-trained model for experimenting, you can train your own network for momentum prediction with the provided data as follows:\r\n\r\n```bash\r\ncd /code/FastPredictiveImageRegistration\r\nth main_nodiff.lua\r\n```\r\n\r\nWe will give another example later for the case when no LDDMM registrations (i.e., the momenta) are available for training.\r\nThe network definition for the particular model in this example can be found in the file `create_model.lua` (in the function `VAE_deformation_parallel_small_noDropout`). Per default, we train (using rmsprop) for 10 epochs using a batch size of 300 and 15x15 patches.\r\n\r\n### Momenta prediction by a trained model\r\n\r\nTo run the (pre)-trained model for momenta prediction, we execute\r\n\r\n```bash\r\ncd /code/FastPredictiveImageRegistration\r\nth test_recon_nodiff.lua\r\n```       \r\n\r\nNote that this code might fail in case you do not have the Torch `matio` module installed. In that\r\ncase, you can use `luarocks` to install it with\r\n\r\n```bash\r\nluarocks install matio\r\n```\r\n        \r\nOnce, `test_recon_nodiff.lua` completes, you should see a file named `2D_output.mat` in the `/code/FastPredictiveImageRegistration` directory. \r\n\r\n### Warping images based on predicted momenta\r\n\r\nWe implement the warping step as follows: first, we will convert all momenta predictions to `.mhd` files; second, we will prepare the (YAML)  configuration files for VectorMomentum LDDMM and eventually run geodesic shooting using the predicted momenta. Lets first start MATLAB and change to the `utils` directory:\r\n\r\n```matlab\r\ncd /code/FastPredictiveImageRegistration/utils\r\n```        \r\n\r\nThen, we edit the file `change_m0_to_mhd_2D.m` and set the corresponding paths. Per default, the output directory will be `/tmp/` and the predicted momenta files will be `/tmp/m_1.mhd` to `/tmp/m_50.mhd`, since we have N=50 test cases. Next, let us write the YAML configuration files for the VectorMomentum LDDMM code. This is done by editing `updateyaml_2D.m`. Running `updateyaml_2D` will then create N=50 configuration files in the output directory (`/tmp` in our case).\r\n\r\n```matlab\r\ncd /code/FastPredictiveImageRegistration/utils\r\nchange_m0_to_mhd_2D\r\nupdateyaml_2D\r\n```        \r\n\r\nWe can now run the VectorMomentum LDDMM code (for our first test case of this example) as follows:\r\n\r\n```bash\r\ncd /code/vectormomentum/Code/Python/Applications/\r\npython CAvmGeodesicShooting.py ~/deformation-prediction-code/utils/deep_network_1.yaml\r\n```\r\n        \r\nThis will create the atlas image *warped* onto the source image in the directory `/tmp/case_1`. In case running the geodesic shooting code produces errors (related to plotting), just comment-out `GeodesicShootingPlots(g, ginv, I0, It, cf)` on line 151 of `CAvmGeodesicShooting.py`.\r\n\r\n## EXAMPLE 2: 3D atlas-to-image registration\r\n\r\nWe provide example 3D training and testing data from the [OASIS](http://www.oasis-brains.org/) \r\nbrain database (including the atlas), i.e.,\r\n\r\n* 100 images for training\r\n* 50 images for testing\r\n* 1 atlas image (in `data/images/3D_atlas/atlas.mhd`)\r\n\r\nThese images have been (affinely) pre-aligned to the atlas (thanks to *Nikhil Singh*). We additionally \r\nprovide momenta (obtained by PyCA) for all training and testing images. This allows us to \r\ntrain and test our encoder-decoder network. \r\n\r\n### Downloading the data\r\n\r\nThe training/testing data can be downloaded [here](https://drive.google.com/drive/folders/0BxHF82gaPzgSemFFbjdDWVN3bkk?usp=sharing.). **Note**: the total data size is approximately 10GB. We recommend to put the data into the `data` subdirectory, i.e., `/code/FastPredictiveImageRegistration/data` in our example, so that you do not have to edit \r\nthe training/testing code. *For now, the paths are hardcoded*.\r\n\r\n### Training\r\n\r\nThe code for training the 3D encoder-decoder network can be found in `main_nodiff_3D_patchprune.lua`, assuming\r\nthat the data has been downloaded to `/code/FastPredictiveImageRegistration/data`. Executing \r\n\r\n```bash\r\ncd /code/FastPredictiveImageRegistration/\r\nth main_nodiff_3D_patchprune.lua\r\n```\r\nwill train the encoder-decoder network (on our system with one NVIDIA Titan X, this takes about two days).\r\n\r\n### Momenta prediction by a trained model\r\n\r\nNext, we will run the model in prediction mode to *map* our N=50 input images to momenta that can be\r\nlater used by the VectorMomentum LDDMM code. This can simply be done by executing\r\n\r\n```bash\r\ncd /code/FastPredictiveImageRegistration/\r\nth test_recon_nodiff_3D.lua\r\nmv 3D_output.mat data/\r\n```\r\nwhich will create a MATLAB file `3D_output.mat` in the directory from which you executed the \r\n`test_recon_nodiff_3D.lua` Torch code, i.e., in our case `/code/FastPredictiveImageRegistration/`.\r\n\r\n### Warping images based on predicted momenta\r\n\r\nOnce the previous step is finished, we need to convert the outputted momenta (in the `output_3D.mat` file) \r\nto `.mhd` files. To do this, we edit `change_m0_to_mhd_3D.m` MATLAB file (in the `utils` subdirectory) \r\nand adjust the variables within `CONFIG::START` and `CONFIG::END` block. In our example, we set:\r\n\r\n```matlab\r\nmomenta_mat_file = '../data/3D_output.mat';\r\noutput_dir = '/tmp/';\r\noutput_prefix = 'm';\r\n``` \r\nThen, we run\r\n\r\n```matlab\r\ncd /code/FastPredictiveImageRegistration/utils\r\nchange_m0_to_mhd_3D\r\n``` \r\n\r\nwhich will create N=50 output files in the `/tmp` folder, named `m_1.mhd`, `m_1.raw`, ..., `m_50.mhd`, `m_50.raw`.\r\nBefore we can run the VectorMomentum LDDMM code on these files, we need to create the required\r\nconfiguration files. As in the 2D example, we edit the `updateyaml_3D.m` MATLAB file and adjust the\r\nvariables within the `CONFIG::START` and `CONFIG::END` block. Eventually, running\r\n\r\n```matlab\r\ncd /code/FastPredictiveImageRegistration/utils\r\nupdateyaml_3D\r\n``` \r\n\r\ngenerates a collection of N=50 `.yaml` files (in the `/tmp` directory) that can be used\r\nto run the VectorMomentum LDDMM code.\r\n\r\nAs in the 2D example, we run the VectorMomentum LDDMM code with\r\n\r\n```bash\r\ncd /external/vectormomentum/Code/Python/Applications/\r\npython CAvmGeodesicShooting.py /tmp/deep_network_1.yaml\r\n```\r\n\r\n## Steps for training/testing our encoder-decoder network on your own data\r\n\r\n*will be available shortly*\r\n\r\n## Acknowledgments\r\n\r\nWe like to thank the creators of the OASIS brain database for freely providing the data.\r\nThe provided (preprocessed) OASIS data is subject to the same **Data Usage Agreement** as listed on the\r\n[OASIS webpage](http://www.oasis-brains.org/app/template/UsageAgreement.vm).\r\n\r\n## Contact\r\n\r\nPlease contact **Xiao Yang** for comments, suggestions or bug reports :)\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}